{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP6jiKg6YNkV"
   },
   "source": [
    "# **CSCI6515 Project** \n",
    "\n",
    "**Group members :**\n",
    "1. Amirul Sunesara - B00813456\n",
    "2. Surabhi Chaudhary - B00826371 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "7L7vOAE_BvxM",
    "outputId": "3bf53929-a55f-4481-e8d1-a7086bbaf71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libaries\n",
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D,UpSampling1D\n",
    "from keras.models import Sequential,Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Reshape\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuYUPunHBvxa"
   },
   "outputs": [],
   "source": [
    "#initiliaze list to store all news data from xml file\n",
    "listNews = []\n",
    "#iterate over all xml files in 'Data' directory and store all fields in list\n",
    "for file in os.listdir('Data'):\n",
    "    if not file.endswith('.xml'): \n",
    "        continue\n",
    "    fileData = os.path.join('Data', file)\n",
    "    tree = et.parse(fileData)\n",
    "    root = tree.getroot()\n",
    "    row = {}\n",
    "    row[\"XMLfilename\"] = file.split('.')[0]\n",
    "    row[\"itemid\"] = root.attrib.get('itemid')\n",
    "    row[\"date\"] = root.attrib.get('date')\n",
    "    \n",
    "    news = ''\n",
    "    for i in root.find('text').iter('p'):\n",
    "        news += i.text\n",
    "    \n",
    "    row[\"text\"] = news\n",
    "    row[\"title\"] = root.find('title').text\n",
    "    row[\"headline\"] = root.find('headline').text\n",
    "    if(root.find('byline') is not None):\n",
    "        row[\"byline\"] =  root.find('byline').text\n",
    "    else:\n",
    "        row[\"byline\"] = np.NaN \n",
    "    if(root.find('dateline') is not None):\n",
    "        row[\"dateline\"] =  root.find('dateline').text\n",
    "    else:\n",
    "        row[\"dateline\"] = np.NaN \n",
    "    row[\"copyright\"] = root.find('copyright').text\n",
    "\n",
    "    for node in root.iter('dc'): \n",
    "        colname = node.attrib.get('element')\n",
    "        colvalue = node.attrib.get('value')\n",
    "        row[colname] = colvalue\n",
    "    \n",
    "    #insert labels\n",
    "    for code in root.iter('codes'):\n",
    "        arrBip = code.attrib.get('class').split(':')\n",
    "        codes_class = arrBip[0]+':'+arrBip[1]\n",
    "        if(codes_class!='bip:topics'):\n",
    "            row[codes_class] = code.find('code').attrib.get('code')\n",
    "    \n",
    "    for code in root.iter('codes'):\n",
    "        arrBip = code.attrib.get('class').split(':')\n",
    "        codes_class = arrBip[0]+':'+arrBip[1]\n",
    "        if(codes_class=='bip:topics'):\n",
    "            for c in code.iter('code'):\n",
    "                rowCopy = row.copy()\n",
    "                rowCopy[codes_class] = c.attrib.get('code')\n",
    "                listNews.append(rowCopy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZqcDiMSBvxl"
   },
   "outputs": [],
   "source": [
    "#convert list to dataframe\n",
    "df = pd.DataFrame(listNews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lXANYPrBvxr"
   },
   "outputs": [],
   "source": [
    "#functions to pre-process the text data, training classifiers\n",
    "def filter_df(df):\n",
    "    return df[[\"headline\",\"text\",\"bip:topics\",\"itemid\",\"dc.date.published\",\"XMLfilename\"]]\n",
    "\n",
    "def getLabels(df):\n",
    "    return df[\"bip:topics\"]\n",
    "\n",
    "def prepare_text_data(df):\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    index=0\n",
    "    for i, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        #remove numbers and non letter characters\n",
    "        text = re.sub(r\"[^a-zA-Z]+\", ' ',text)\n",
    "        #convert text to lower case\n",
    "        text = text.lower() \n",
    "        #remove stop words\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        #lematize words\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        df.loc[i,'text'] = text\n",
    "        index=index+1\n",
    "    return df\n",
    "\n",
    "def generate_features_and_labels(df):\n",
    "    newDf=pd.DataFrame()\n",
    "    newDf[\"labels\"] = getLabels(df)\n",
    "    newDf[\"features\"] = df[\"text\"]\n",
    "    newDf = newDf.dropna()\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    features = tfidf_vectorizer.fit_transform(newDf[\"features\"])\n",
    "    #svd = TruncatedSVD(2)\n",
    "    #transformed = svd.fit_transform(features)\n",
    "    return newDf,features\n",
    "\n",
    "def train_classfier(modelName,df):\n",
    "    print(\"----\"+modelName+\"----\")\n",
    "    newDf,features = generate_features_and_labels(df)\n",
    "    labels = newDf[\"labels\"]\n",
    "    print(\"feature size\",features.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size=0.3,random_state=42)\n",
    "    if modelName == 'Random Forest':\n",
    "        clf_rf = RandomForestClassifier()\n",
    "        clf_rf.fit(X_train, y_train)\n",
    "        pred = clf_rf.predict(X_test)\n",
    "        cross_validation = cross_val_score(clf_rf,features,labels,cv=5)\n",
    "        print(\"cross-validation score: \" +str(cross_validation.mean()))\n",
    "    elif modelName == 'Decision Tree':\n",
    "        grid_param={'min_samples_split' : [10,50,100],'max_depth': [2,4,8]}\n",
    "        clf_dt = GridSearchCV(DecisionTreeClassifier(),param_grid=grid_param,cv=5,n_jobs=-1)\n",
    "        clf_dt.fit(X_train,y_train)\n",
    "        pred = clf_dt.predict(X_test)\n",
    "        cross_validation = cross_val_score(clf_dt,features,labels,cv=5)\n",
    "        print(\"cross-validation score: \" +str(cross_validation.mean()))\n",
    "    elif modelName == 'Gradient Boost':\n",
    "        #grid_param={'n_estimators':[100,200,300],'min_samples_split':[2,4,6]}\n",
    "        #clf_gb = GridSearchCV(GradientBoostingClassifier(),param_grid=grid_param,cv=5,n_jobs=-1)\n",
    "        clf_gb = GradientBoostingClassifier()\n",
    "        clf_gb.fit(X_train,y_train)\n",
    "        pred = clf_gb.predict(X_test)\n",
    "        cross_validation = cross_val_score(clf_gb,features,labels,cv=5)\n",
    "        print(\"cross-validation score: \" +str(cross_validation.mean()))\n",
    "    elif modelName == 'Neural Network':\n",
    "        grid_param={'alpha': [0.0001,0.03],'learning_rate': ['constant','adaptive']}\n",
    "        clf_nn = GridSearchCV(MLPClassifier(),param_grid=grid_param,cv=5,n_jobs=-1)\n",
    "        clf_nn.fit(X_train,y_train)\n",
    "        pred = clf_nn.predict(X_test)\n",
    "        cross_validation = cross_val_score(clf_nn,features,labels,cv=5)\n",
    "        print(\"cross-validation score: \" +str(cross_validation.mean()))\n",
    "    elif modelName == 'SVM':\n",
    "        clf_svm = SVC(kernel='linear')\n",
    "        clf_svm.fit(X_train, y_train)\n",
    "        pred = clf_svm.predict(X_test)\n",
    "        cross_validation = cross_val_score(clf_svm,features,labels,cv=5)\n",
    "        print(\"cross-validation score: \" +str(cross_validation.mean()))\n",
    "    \n",
    "    return y_test,pred\n",
    "\n",
    "def evaluate_classfier_quality(pred,y_test):\n",
    "    accuracy = metrics.accuracy_score(y_test,pred)\n",
    "    print(\"Accuracy: %1.4f\" %accuracy)\n",
    "    precision = metrics.precision_score(y_test,pred,average='weighted')\n",
    "    print(\"Precision: %1.4f\" % precision)\n",
    "    recall = metrics.recall_score(y_test,pred,average='weighted')\n",
    "    print(\"Recall: %1.4f\" % recall)\n",
    "    f1 = metrics.f1_score(y_test,pred,average='weighted')\n",
    "    print(\"F1: %1.4f\" % f1)\n",
    "    return accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "AgeXnms9waXH",
    "outputId": "194f880e-9ed4-4acd-c672-2269d9634a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrrDpCM5Bvxz"
   },
   "outputs": [],
   "source": [
    "#calling the function to pre-process the data and saving it to a csv file\n",
    "df = filter_df(df)\n",
    "df = prepare_text_data(df)\n",
    "df.to_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebbh7ZdlU86j"
   },
   "source": [
    "## Loading the dataset from Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LR5RifCuVHaM"
   },
   "source": [
    "**Reducing the features beforing clustering** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "S9ai6sGeBvx4",
    "outputId": "0d3c7880-3fe1-4f42-a726-6ca93833ca90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46228, 101404)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('drive/My Drive/data/MLBD_Project/processed_data.csv')\n",
    "df.drop_duplicates(keep=\"first\",subset=\"text\", inplace=True)\n",
    "df = df.dropna()\n",
    "newDf,features = generate_features_and_labels(df)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_XjeGNwU87O"
   },
   "source": [
    "## Clustering the documents using k-means classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mm5qswCuoy7a"
   },
   "source": [
    "On “text” column of dataset, pre-processing is done wherein after cleaning, tf-idf vectorizer has been done. Tf-idf is chosen by us since it gives importance of word with respect to each document irrespective of just the word counts. Here, we are doing processing on dataframe of shape 46228 * 6 and after tf-idf the shape has been changes to 46228 * 101404. Since we are getting large number of features after tf-idf it is wise to perform dimensionality reduction at this phase. Hence, we are using TruncatedSVD, to reduce the dimensions for smooth clustering process. Before, dimentionality reduction we performed clustering, but it was taking too much because of huge number of dimention. Trucated SVD is like PCA technique, the only difference is that it performs operation of data matrix instead of covariance matrix.  \n",
    "\n",
    "**Clustering the documents**\n",
    "\n",
    "To cluster all documents, K-means algorithm has been used. **Elbow** method has been used identify optimum number of clusters for K-means algorithm. In elbow method, elbow formation can be seen after 8 number of clusters i.e. sum of squared error is decreasing very slowly after 8 clusters. Thus, 8 clusters are considered for further processing. After clustering, cluster id is assigned to each document in the original dataset. After that Random Forest has been used for each of the cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qoFHxulQeb0"
   },
   "outputs": [],
   "source": [
    "#Using truncated SVD to to reduce the features to perform efficient and time efficient clustering \n",
    "n_component_val = 15\n",
    "tsvd = TruncatedSVD(n_components=n_component_val)\n",
    "featuresSVD = tsvd.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "AAlkVPVVBvyN",
    "outputId": "ed31ce10-636d-4d1e-98a1-9928211059df"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFzCAYAAADR6BVMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Elbow method\n",
    "sse = []\n",
    "num_of_k = list(range(2, 16))\n",
    "\n",
    "for k in num_of_k:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(features)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pt5x9vrbU87h"
   },
   "source": [
    "### Performing Clustering and Assigning cluster id to each document\n",
    "\n",
    "Once we get cluster id from K-means, it has been assigned to respective documents, in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "PrXVUzSaTYWS",
    "outputId": "107bc446-c83d-46a2-c3a6-5407d92212b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original features :  (46228, 101404)\n",
      "length of y_pred of k means cluster :  46228\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>bip:topics</th>\n",
       "      <th>itemid</th>\n",
       "      <th>dc.date.published</th>\n",
       "      <th>XMLfilename</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian Occidental mounts rival Wascana bid.</td>\n",
       "      <td>canadian occidental petroleum ltd emerged tues...</td>\n",
       "      <td>C181</td>\n",
       "      <td>326914</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326914newsML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gruma, Maseca to receive syndicated loan - bank.</td>\n",
       "      <td>bank america launch three year million syndica...</td>\n",
       "      <td>C173</td>\n",
       "      <td>326915</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326915newsML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Too early to call Krupp bid hostile - Deutsche...</td>\n",
       "      <td>deutsche bank ag management board member rolf ...</td>\n",
       "      <td>C18</td>\n",
       "      <td>326916</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326916newsML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FOCUS - Euro bourses fret over Wall St, electi...</td>\n",
       "      <td>european bourse fell tuesday even wall street ...</td>\n",
       "      <td>M11</td>\n",
       "      <td>326917</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326917newsML</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>French stocks fall, Alcatel posts big gain.</td>\n",
       "      <td>french share closed lower tuesday second conse...</td>\n",
       "      <td>G152</td>\n",
       "      <td>326918</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326918newsML</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156405</th>\n",
       "      <td>Bulgaria seeks advisers for sale of state firms.</td>\n",
       "      <td>bulgarian government seeking financial adviser...</td>\n",
       "      <td>C183</td>\n",
       "      <td>477877</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477877newsML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156406</th>\n",
       "      <td>INTERVIEW-Lion Teck Chiang sees net up 70 pct ...</td>\n",
       "      <td>property steel group lion teck chiang ltd part...</td>\n",
       "      <td>C152</td>\n",
       "      <td>477878</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477878newsML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156411</th>\n",
       "      <td>Indian shares plunge 8.6 pct on political crisis.</td>\n",
       "      <td>indian share plunged eight percent panicky tra...</td>\n",
       "      <td>M11</td>\n",
       "      <td>477882</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477882newsML</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156412</th>\n",
       "      <td>Singapore shares open weak, funds stay sidelined.</td>\n",
       "      <td>singapore share opened weaker monday fund mana...</td>\n",
       "      <td>M11</td>\n",
       "      <td>477883</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477883newsML</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156414</th>\n",
       "      <td>Tokyo stocks down in thin morning trade.</td>\n",
       "      <td>tokyo stock fell monday morning last day busin...</td>\n",
       "      <td>M11</td>\n",
       "      <td>477885</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477885newsML</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46228 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  ... cluster_id\n",
       "0           Canadian Occidental mounts rival Wascana bid.  ...          1\n",
       "1        Gruma, Maseca to receive syndicated loan - bank.  ...          1\n",
       "2       Too early to call Krupp bid hostile - Deutsche...  ...          1\n",
       "5       FOCUS - Euro bourses fret over Wall St, electi...  ...          6\n",
       "11            French stocks fall, Alcatel posts big gain.  ...          6\n",
       "...                                                   ...  ...        ...\n",
       "156405   Bulgaria seeks advisers for sale of state firms.  ...          1\n",
       "156406  INTERVIEW-Lion Teck Chiang sees net up 70 pct ...  ...          1\n",
       "156411  Indian shares plunge 8.6 pct on political crisis.  ...          0\n",
       "156412  Singapore shares open weak, funds stay sidelined.  ...          6\n",
       "156414           Tokyo stocks down in thin morning trade.  ...          6\n",
       "\n",
       "[46228 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performing clustering using Kmean classifier on features and obtaining the cluster id\n",
    "def cluster_data():\n",
    "  y_pred = KMeans(n_clusters=8).fit_predict(featuresSVD)\n",
    "  print(\"Shape of original features : \",features.shape)\n",
    "  print(\"length of y_pred of k means cluster : \", len(y_pred))\n",
    "  df[\"cluster_id\"]=y_pred\n",
    "\n",
    "cluster_date()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bm2NGJh4U876"
   },
   "source": [
    "## Different classifier for each identified cluster\n",
    "For each of identified clusters, Random Forest classifier has been used. \n",
    "In total, 8 clusters are identified and classifier is used on each of them. \n",
    "\n",
    "**Analysis :** Out of these clusters, most of the clusters are perfoeming good except for two clausters where the performance goes in range of 60% for all of these classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DOBZy_5EBvyU",
    "outputId": "552df9ec-55ff-44a1-b95b-e49e42701d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================== Cluster : 6 ============================\n",
      "----Random Forest----\n",
      "feature size (9034, 32400)\n",
      "cross-validation score: 0.6439187034638251\n",
      "Accuracy: 0.6356\n",
      "Precision: 0.6375\n",
      "Recall: 0.6356\n",
      "F1: 0.5803\n",
      "\n",
      "\n",
      "===================== Cluster : 1 ============================\n",
      "----Random Forest----\n",
      "feature size (22493, 80499)\n",
      "cross-validation score: 0.6089061511666657\n",
      "Accuracy: 0.6180\n",
      "Precision: 0.5921\n",
      "Recall: 0.6180\n",
      "F1: 0.5791\n",
      "\n",
      "\n",
      "===================== Cluster : 0 ============================\n",
      "----Random Forest----\n",
      "feature size (5281, 20881)\n",
      "cross-validation score: 0.6247576255618165\n",
      "Accuracy: 0.6215\n",
      "Precision: 0.6115\n",
      "Recall: 0.6215\n",
      "F1: 0.5978\n",
      "\n",
      "\n",
      "===================== Cluster : 5 ============================\n",
      "----Random Forest----\n",
      "feature size (3263, 13952)\n",
      "cross-validation score: 0.7877771822485974\n",
      "Accuracy: 0.7906\n",
      "Precision: 0.7653\n",
      "Recall: 0.7906\n",
      "F1: 0.7495\n",
      "\n",
      "\n",
      "===================== Cluster : 7 ============================\n",
      "----Random Forest----\n",
      "feature size (2713, 11100)\n",
      "cross-validation score: 0.8413509623786032\n",
      "Accuracy: 0.8428\n",
      "Precision: 0.8430\n",
      "Recall: 0.8428\n",
      "F1: 0.8311\n",
      "\n",
      "\n",
      "===================== Cluster : 4 ============================\n",
      "----Random Forest----\n",
      "feature size (2186, 6159)\n",
      "cross-validation score: 0.9926876849429318\n",
      "Accuracy: 0.9924\n",
      "Precision: 0.9848\n",
      "Recall: 0.9924\n",
      "F1: 0.9886\n",
      "\n",
      "\n",
      "===================== Cluster : 2 ============================\n",
      "----Random Forest----\n",
      "feature size (602, 7308)\n",
      "cross-validation score: 0.9346556266402543\n",
      "Accuracy: 0.8895\n",
      "Precision: 0.7912\n",
      "Recall: 0.8895\n",
      "F1: 0.8375\n",
      "\n",
      "\n",
      "===================== Cluster : 3 ============================\n",
      "----Random Forest----\n",
      "feature size (656, 1823)\n",
      "cross-validation score: 0.9716446610268378\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9698\n",
      "Recall: 0.9848\n",
      "F1: 0.9772\n"
     ]
    }
   ],
   "source": [
    "#Random forest classifier using cross val technique\n",
    "for n in df[\"cluster_id\"].unique():\n",
    "  print(\"\\n\\n===================== Cluster : \"+str(n)+\" ============================\")\n",
    "  y_test,pred = train_classfier('Random Forest',df.loc[df[\"cluster_id\"]==n])\n",
    "  rf_accuracy,rf_precision,rf_recall,svm_f1 = evaluate_classfier_quality(pred,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La76AyD6U88Y"
   },
   "source": [
    "## Evaluating the quality of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGKco9drpoS0"
   },
   "source": [
    "We used elbow method to determine optimal number of clusters for k means algorithm. For same number of clusters, to determine the quality of clusters Silhouette's score has been used. \n",
    "\n",
    "**Silhoeutte's method**\n",
    "\n",
    "To evaluate the quality of clusters, Silhoeuttes method has been used. \n",
    "\n",
    "In this step, clusters are generated using K-means algorithm and then, silhoeutte score is calculated for number of clusters = 8 which was identified in elbow result. The more the Silhoutte score is, better the cluster is. \n",
    "In our case, Silhoutte score was maximum for number of cluster = 8 and Silhouette score is 0.47 which is considerable. However, in our case Silhouettes score keeps on fluctuating with more number of clusters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Zrhp_wu6NOZk",
    "outputId": "8c2ba075-a979-47ec-ad0a-20228518f855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score =  0.4721952351112363\n"
     ]
    }
   ],
   "source": [
    "#Silhoutte's score calculation\n",
    "def check_cluster_quality():\n",
    "\n",
    "  kmeans = KMeans (n_clusters=8)\n",
    "  preds = kmeans.fit_predict(featuresSVD)\n",
    "  centers = kmeans.cluster_centers_\n",
    "  #calculating the Silhoeutte score for each number of clusters\n",
    "  score = silhouette_score(featuresSVD, preds,metric='euclidean')\n",
    "  print (\"Silhouette score = \", score)\n",
    "\n",
    "check_cluster_quality()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2zgaaWuhMQB"
   },
   "source": [
    "## Feature extraction\n",
    "Two methods for feature extraction has been used and the reference for this knowledge has been taken from \"Text feature extraction based on deep learning: a review\" research paper.\n",
    "1. Autoencoder\n",
    "2. CNN\n",
    "\n",
    "### i) Autoencoder\n",
    "\n",
    "Since we were having, 101404 features, it becomes a necessity to choose the best features and restrict number of features. \n",
    "We have tried stacked Autoencoder, which is accuracy of 99.8% when decoding the output and comparing it to input provided. Hence we can say that, compressed data from autoencoder represents the input in best possible way. \n",
    "\n",
    "We have used three dense encoded and decoded layers and activation function is relu. We have used batch size of 256 to increase training speed and binary cross entropy loss function has been used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKPNSdAKd5-F"
   },
   "outputs": [],
   "source": [
    "arrAEFeatures = []\n",
    "\n",
    "def build_AE(X_train,X_test):\n",
    "    input_data = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    encoded = Sequential()\n",
    "    #Encoder\n",
    "    encoded = Dense(256, activation='relu',name=\"encode_layer1\")(input_data)\n",
    "    encoded = Dense(128, activation='relu',name=\"encode_layer2\")(encoded)\n",
    "    encoded = Dense(64, activation='relu',name=\"encode_layer3\")(encoded)\n",
    "\n",
    "    #Decoder\n",
    "    decoded = Dense(128, activation='relu',name=\"decode_layer1\")(encoded)\n",
    "    decoded = Dense(256, activation='relu',name=\"decode_layer2\")(decoded)\n",
    "    decoded = Dense(X_train.shape[1], activation='sigmoid',name=\"decode_layer3\")(decoded)\n",
    "\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer='adadelta',metrics=['accuracy'], loss='binary_crossentropy')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder\n",
    "\n",
    "def run_autoEncoder():\n",
    "    #extract features from each cluster and store in array\n",
    "    for n in df[\"cluster_id\"].unique():\n",
    "        dictClusterFeatures = {}\n",
    "\n",
    "        #get dataframe of current cluster\n",
    "        currentDf = df.loc[df[\"cluster_id\"]==n]\n",
    "\n",
    "        #get data in tfidf vectorized form\n",
    "        newDf,X = generate_features_and_labels(currentDf)\n",
    "        Y = newDf[\"labels\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "      \n",
    "        autoencoder = build_AE(X_train,X_test)\n",
    "        autoencoder.fit(X_train, X_train,epochs=5,batch_size=256,validation_data=(X_test, X_test))\n",
    "        \n",
    "        #extract feautures from intermediate layer\n",
    "        middle_layer = Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('encode_layer3').output)\n",
    "        extractedFeatures = middle_layer.predict(X_train)\n",
    "        \n",
    "        #insert extracted feautes in dictionery\n",
    "        dictClusterFeatures[\"cluster_id\"] = n\n",
    "        dictClusterFeatures[\"features\"] = extractedFeatures\n",
    "        dictClusterFeatures[\"target\"] = y_train\n",
    "        arrAEFeatures.append(dictClusterFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6tBMV86S1I-_",
    "outputId": "b6a99904-1dbe-49ed-ac0b-d4e514624669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 59517)             0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               15236608  \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 59517)             15295869  \n",
      "=================================================================\n",
      "Total params: 30,614,973\n",
      "Trainable params: 30,614,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 15373 samples, validate on 3844 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "15373/15373 [==============================] - 121s 8ms/step - loss: 0.6930 - acc: 0.9305 - val_loss: 0.6929 - val_acc: 0.9988\n",
      "Epoch 2/5\n",
      "15373/15373 [==============================] - 118s 8ms/step - loss: 0.6928 - acc: 0.9988 - val_loss: 0.6926 - val_acc: 0.9988\n",
      "Epoch 3/5\n",
      "15373/15373 [==============================] - 118s 8ms/step - loss: 0.6925 - acc: 0.9988 - val_loss: 0.6924 - val_acc: 0.9988\n",
      "Epoch 4/5\n",
      "15373/15373 [==============================] - 119s 8ms/step - loss: 0.6923 - acc: 0.9988 - val_loss: 0.6921 - val_acc: 0.9988\n",
      "Epoch 5/5\n",
      "15373/15373 [==============================] - 118s 8ms/step - loss: 0.6920 - acc: 0.9988 - val_loss: 0.6919 - val_acc: 0.9988\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 22986)             0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               5884672   \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 22986)             5907402   \n",
      "=================================================================\n",
      "Total params: 11,874,570\n",
      "Trainable params: 11,874,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5153 samples, validate on 1289 samples\n",
      "Epoch 1/5\n",
      "5153/5153 [==============================] - 17s 3ms/step - loss: 0.6930 - acc: 0.8101 - val_loss: 0.6929 - val_acc: 0.9780\n",
      "Epoch 2/5\n",
      "5153/5153 [==============================] - 16s 3ms/step - loss: 0.6928 - acc: 0.9905 - val_loss: 0.6927 - val_acc: 0.9951\n",
      "Epoch 3/5\n",
      "5153/5153 [==============================] - 16s 3ms/step - loss: 0.6926 - acc: 0.9954 - val_loss: 0.6925 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "5153/5153 [==============================] - 16s 3ms/step - loss: 0.6923 - acc: 0.9956 - val_loss: 0.6922 - val_acc: 0.9957\n",
      "Epoch 5/5\n",
      "5153/5153 [==============================] - 16s 3ms/step - loss: 0.6921 - acc: 0.9956 - val_loss: 0.6920 - val_acc: 0.9957\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 13854)             0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               3546880   \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 13854)             3560478   \n",
      "=================================================================\n",
      "Total params: 7,189,854\n",
      "Trainable params: 7,189,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2542 samples, validate on 636 samples\n",
      "Epoch 1/5\n",
      "2542/2542 [==============================] - 6s 2ms/step - loss: 0.6931 - acc: 0.6574 - val_loss: 0.6930 - val_acc: 0.8091\n",
      "Epoch 2/5\n",
      "2542/2542 [==============================] - 5s 2ms/step - loss: 0.6929 - acc: 0.8727 - val_loss: 0.6928 - val_acc: 0.9258\n",
      "Epoch 3/5\n",
      "2542/2542 [==============================] - 5s 2ms/step - loss: 0.6927 - acc: 0.9465 - val_loss: 0.6926 - val_acc: 0.9629\n",
      "Epoch 4/5\n",
      "2542/2542 [==============================] - 5s 2ms/step - loss: 0.6925 - acc: 0.9706 - val_loss: 0.6924 - val_acc: 0.9763\n",
      "Epoch 5/5\n",
      "2542/2542 [==============================] - 5s 2ms/step - loss: 0.6923 - acc: 0.9797 - val_loss: 0.6922 - val_acc: 0.9817\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 61349)             0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               15705600  \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 61349)             15766693  \n",
      "=================================================================\n",
      "Total params: 31,554,789\n",
      "Trainable params: 31,554,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10843 samples, validate on 2711 samples\n",
      "Epoch 1/5\n",
      "10843/10843 [==============================] - 90s 8ms/step - loss: 0.6931 - acc: 0.8936 - val_loss: 0.6930 - val_acc: 0.9972\n",
      "Epoch 2/5\n",
      "10843/10843 [==============================] - 89s 8ms/step - loss: 0.6929 - acc: 0.9977 - val_loss: 0.6928 - val_acc: 0.9978\n",
      "Epoch 3/5\n",
      "10843/10843 [==============================] - 89s 8ms/step - loss: 0.6927 - acc: 0.9978 - val_loss: 0.6926 - val_acc: 0.9978\n",
      "Epoch 4/5\n",
      "10843/10843 [==============================] - 89s 8ms/step - loss: 0.6925 - acc: 0.9978 - val_loss: 0.6924 - val_acc: 0.9978\n",
      "Epoch 5/5\n",
      "10843/10843 [==============================] - 88s 8ms/step - loss: 0.6924 - acc: 0.9978 - val_loss: 0.6923 - val_acc: 0.9978\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 6863)              0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               1757184   \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 6863)              1763791   \n",
      "=================================================================\n",
      "Total params: 3,603,471\n",
      "Trainable params: 3,603,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1222 samples, validate on 306 samples\n",
      "Epoch 1/5\n",
      "1222/1222 [==============================] - 2s 2ms/step - loss: 0.6931 - acc: 0.5645 - val_loss: 0.6930 - val_acc: 0.6688\n",
      "Epoch 2/5\n",
      "1222/1222 [==============================] - 1s 920us/step - loss: 0.6929 - acc: 0.7239 - val_loss: 0.6928 - val_acc: 0.8002\n",
      "Epoch 3/5\n",
      "1222/1222 [==============================] - 1s 914us/step - loss: 0.6927 - acc: 0.8355 - val_loss: 0.6926 - val_acc: 0.8813\n",
      "Epoch 4/5\n",
      "1222/1222 [==============================] - 1s 906us/step - loss: 0.6925 - acc: 0.9010 - val_loss: 0.6924 - val_acc: 0.9259\n",
      "Epoch 5/5\n",
      "1222/1222 [==============================] - 1s 913us/step - loss: 0.6923 - acc: 0.9366 - val_loss: 0.6922 - val_acc: 0.9498\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 7190)              0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               1840896   \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 7190)              1847830   \n",
      "=================================================================\n",
      "Total params: 3,771,222\n",
      "Trainable params: 3,771,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 470 samples, validate on 118 samples\n",
      "Epoch 1/5\n",
      "470/470 [==============================] - 1s 3ms/step - loss: 0.6931 - acc: 0.5137 - val_loss: 0.6931 - val_acc: 0.5732\n",
      "Epoch 2/5\n",
      "470/470 [==============================] - 0s 1ms/step - loss: 0.6931 - acc: 0.5890 - val_loss: 0.6930 - val_acc: 0.6459\n",
      "Epoch 3/5\n",
      "470/470 [==============================] - 0s 997us/step - loss: 0.6930 - acc: 0.6576 - val_loss: 0.6929 - val_acc: 0.7090\n",
      "Epoch 4/5\n",
      "470/470 [==============================] - 0s 1ms/step - loss: 0.6929 - acc: 0.7174 - val_loss: 0.6929 - val_acc: 0.7626\n",
      "Epoch 5/5\n",
      "470/470 [==============================] - 0s 979us/step - loss: 0.6928 - acc: 0.7676 - val_loss: 0.6928 - val_acc: 0.8063\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 3181)              0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               814592    \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 3181)              817517    \n",
      "=================================================================\n",
      "Total params: 1,714,605\n",
      "Trainable params: 1,714,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 852 samples, validate on 213 samples\n",
      "Epoch 1/5\n",
      "852/852 [==============================] - 1s 2ms/step - loss: 0.6930 - acc: 0.5615 - val_loss: 0.6928 - val_acc: 0.6936\n",
      "Epoch 2/5\n",
      "852/852 [==============================] - 0s 501us/step - loss: 0.6927 - acc: 0.7396 - val_loss: 0.6925 - val_acc: 0.8272\n",
      "Epoch 3/5\n",
      "852/852 [==============================] - 0s 500us/step - loss: 0.6924 - acc: 0.8522 - val_loss: 0.6921 - val_acc: 0.8977\n",
      "Epoch 4/5\n",
      "852/852 [==============================] - 0s 506us/step - loss: 0.6920 - acc: 0.9095 - val_loss: 0.6918 - val_acc: 0.9315\n",
      "Epoch 5/5\n",
      "852/852 [==============================] - 0s 492us/step - loss: 0.6917 - acc: 0.9370 - val_loss: 0.6915 - val_acc: 0.9480\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 1857)              0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 256)               475648    \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "encode_layer3 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decode_layer1 (Dense)        (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "decode_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer3 (Dense)        (None, 1857)              477249    \n",
      "=================================================================\n",
      "Total params: 1,035,393\n",
      "Trainable params: 1,035,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 524 samples, validate on 132 samples\n",
      "Epoch 1/5\n",
      "524/524 [==============================] - 1s 2ms/step - loss: 0.6931 - acc: 0.5126 - val_loss: 0.6927 - val_acc: 0.6266\n",
      "Epoch 2/5\n",
      "524/524 [==============================] - 0s 345us/step - loss: 0.6926 - acc: 0.6503 - val_loss: 0.6923 - val_acc: 0.7436\n",
      "Epoch 3/5\n",
      "524/524 [==============================] - 0s 343us/step - loss: 0.6922 - acc: 0.7590 - val_loss: 0.6918 - val_acc: 0.8214\n",
      "Epoch 4/5\n",
      "524/524 [==============================] - 0s 344us/step - loss: 0.6917 - acc: 0.8303 - val_loss: 0.6914 - val_acc: 0.8681\n",
      "Epoch 5/5\n",
      "524/524 [==============================] - 0s 355us/step - loss: 0.6913 - acc: 0.8734 - val_loss: 0.6909 - val_acc: 0.8953\n"
     ]
    }
   ],
   "source": [
    "run_autoEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhJ9ERBhPIt2"
   },
   "source": [
    "### ii) CNN for feature extraction (Extra work)\n",
    "We tried to implement CNN for feature extraction but it failed because of few errors\n",
    "\n",
    "We can take advantage of convolutional layer to find the pattern in text corpus. We have used total of 4 convolution layers with dropout layer to drop the fraction of 0.5 to escape from overfitting. This layer is essentially important because we are extracting features having more importance and regularizing the model. \n",
    "\n",
    "Word embedding:\n",
    "This layer has been used as the first layer of model.\n",
    "\n",
    "Glove has been used to generate word embedding for our features and it works best with CNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkncUQ5xp5Lj"
   },
   "outputs": [],
   "source": [
    "arrCNNFeatures = []\n",
    "#this function tokenize corpus and specify indexes of words\n",
    "def tokenize_data(X_train, X_test):\n",
    "    train_test = np.concatenate((X_train, X_test), axis=0)\n",
    "    arrTrainText = np.array(train_test)\n",
    "    ObjTokenizer = Tokenizer(num_words=75000)\n",
    "    ObjTokenizer.fit_on_texts(arrTrainText)\n",
    "    seq = ObjTokenizer.texts_to_sequences(arrTrainText)\n",
    "    w_index = ObjTokenizer.word_index\n",
    "    seqText = pad_sequences(seq, maxlen=500)\n",
    "    ind = np.arange(seqText.shape[0])\n",
    "    lenX = len(X_train)\n",
    "    textInstance = seqText[ind]\n",
    "    X_train = textInstance[0:lenX, ]\n",
    "    X_test = textInstance[lenX:, ]\n",
    "    return X_train,X_test,w_index\n",
    "\n",
    "#this function fetch embedding from glove list\n",
    "def get_glove_embedding():\n",
    "    index_embedding = {}\n",
    "    file = open(\"drive/My Drive/data/MLBD_Project/glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        arrWords = line.split()\n",
    "        word = arrWords[0]\n",
    "        try:\n",
    "            coff = np.asarray(arrWords[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        index_embedding[word] = coff\n",
    "    file.close()\n",
    "    return index_embedding\n",
    "\n",
    "#build convolutional neural network \n",
    "def build_CNN(w_index, index_embedding, num_of_classes):\n",
    "    convolutions = []\n",
    "    seqInput = Input(shape=(500,), dtype='int32')\n",
    "    cnn = Sequential()\n",
    "    lenWordIndex = len(w_index) + 1\n",
    "    arrEmbedding = np.random.random((len(w_index) + 1, 50))\n",
    "    for word, i in w_index.items():\n",
    "        vector = index_embedding.get(word)\n",
    "        if vector is not None:\n",
    "            if len(arrEmbedding[i]) !=len(vector):\n",
    "                exit(1)\n",
    "            arrEmbedding[i] = vector\n",
    "    layerEmbedding = Embedding(lenWordIndex,50,weights=[arrEmbedding],input_length=500,trainable=True)\n",
    "    seqEmbedding = layerEmbedding(seqInput)\n",
    "\n",
    "    for ks in [5,6]:\n",
    "        conv = Conv1D(128, activation='relu',kernel_size=ks)(seqEmbedding)\n",
    "        pool = MaxPooling1D(5)(conv)\n",
    "        convolutions.append(pool)\n",
    "\n",
    "    mergeConv = Concatenate(axis=1)(convolutions)\n",
    "    conv = Conv1D(128, 5, activation='relu')(mergeConv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    pool = MaxPooling1D(5)(conv)\n",
    "    conv2 = Conv1D(128, 5, activation='relu')(pool)\n",
    "    conv2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(30)(conv2)\n",
    "    flat = Flatten()(pool2)\n",
    "    dense = Dense(1024, activation='relu')(flat)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    dense = Dense(512, activation='relu',name=\"feature_layer\")(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    prediction = Dense(num_of_classes, activation='softmax')(dense)\n",
    "    cnn = Model(seqInput, prediction)\n",
    "    cnn.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return cnn\n",
    "\n",
    "\n",
    "def extract_feautres_cnn():\n",
    "  \n",
    "  for n in df[\"cluster_id\"].unique():\n",
    "    dictClusterFeatures = {}\n",
    "\n",
    "    #get dataframe of current cluster\n",
    "    currentDf = df.loc[df[\"cluster_id\"]==n]\n",
    "    #passing features of each and every cluster to neural network\n",
    "    print(\"\\n\\Extracting features from convolutional neural network\")\n",
    "    \n",
    "\n",
    "    classes = currentDf[\"bip:topics\"].unique().tolist()\n",
    "    #encode classes to numeric values\n",
    "    target = currentDf[\"bip:topics\"].apply(classes.index)\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(currentDf[\"text\"],target,test_size=0.3,random_state=42)\n",
    "    X_train_new,X_test_new,word_index = tokenize_data(X_train,X_test)\n",
    "    embeddings_index = get_glove_embedding()\n",
    "    cnn = build_CNN(word_index,embeddings_index, len(target.unique()))\n",
    "\n",
    "    cnn.summary()\n",
    "\n",
    "    cnn.fit(X_train_new, y_train,validation_data=(X_test_new, y_test),epochs=10,batch_size=256,verbose=2)\n",
    "\n",
    "    predicted = cnn.predict(X_train_new)\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "    #extract feautures from intermediate layer\n",
    "    middle_layer = Model(inputs=cnn.input,outputs=cnn.get_layer('feature_layer').output)\n",
    "    extractedFeatures = middle_layer.predict(X_train)\n",
    "        \n",
    "    #insert extracted feautes in dictionery\n",
    "    dictCNNFeatures[\"cluster_id\"] = n\n",
    "    dictCNNFeatures[\"features\"] = extractedFeatures\n",
    "    dictCNNFeatures[\"target\"] = y_train\n",
    "    arrAEFeatures.append(dictCNNFeatures)\n",
    "\n",
    "    print(metrics.classification_report(y_train, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0MvzsbWhbXc"
   },
   "source": [
    "## Passing new features in deep neural networks\n",
    "\n",
    "At this stage, features extracted from stacked autoencoder has been been used for Neural network training. Here we are using Sequential neural network to understand how new features extracted from stacked autoencoder improving the efficiency of classification. \n",
    "Since there are total of 8 clusters, one of the cluster is taken into consideration to represent the improved accuracy with Neural network and new extracted features.\n",
    "\n",
    "\n",
    "**Before Feature Extraction**\n",
    "\n",
    "Random Forest : average accuracy 70.25%\n",
    "Classification was performed on all the clusters and above mentioned is the average accuracy. \n",
    "\n",
    "Accuracy of cluster 8 : 84%\n",
    "\n",
    "**After feature extraction**\n",
    "\n",
    "Sequential model: average accuacy 79.50%\n",
    "\n",
    "Accuracy of cluster 8 : 93.6%\n",
    "\n",
    "\n",
    "Hence it can be seen that classification has been improved with the new features. Moreover, it is important to notice that classifers Random Forest is trained using 101404 features in comparision to sequential model(trained on only one third of features) and giving better performance. \n",
    "With the use of NN,more meaningful features used for training the NN classifier and here, Autoencoder is giving significant results while extracting the features. Sequential model is using multiple layer to learn maximum from features in each cluster.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFZ5T605AcP_"
   },
   "outputs": [],
   "source": [
    "def build_deep_neural_network(shape, num_of_classes):\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(512,input_dim=shape,activation='relu'))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(512,input_dim=512,activation='relu'))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(512,input_dim=512,activation='relu'))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(512,input_dim=512,activation='relu'))\n",
    "    nn.add(Dropout(0.5))\n",
    "    print(\"num_of_classes\",num_of_classes)\n",
    "    nn.add(Dense(num_of_classes+num_of_classes, activation='softmax'))\n",
    "    nn.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "def run_deep_neural_network():\n",
    "  #passing features of each and every cluster to neural network\n",
    "  for dictFeatures in arrAEFeatures:\n",
    "    print(\"\\n\\nTraining neural network for Cluster id: \"+str(dictFeatures[\"cluster_id\"]))\n",
    "    target = dictFeatures[\"target\"]\n",
    "    classes = target.unique().tolist()\n",
    "    #encode classes to numeric values\n",
    "    target = dictFeatures[\"target\"].apply(classes.index)\n",
    "    features = dictFeatures[\"features\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,target,test_size=0.3,random_state=42)\n",
    "    nn = build_deep_neural_network(X_train.shape[1],len(y_train.unique())+1)\n",
    "    nn.fit(X_train, y_train,validation_data=(X_test, y_test),epochs=10,batch_size=128,verbose=2)\n",
    "    predicted = nn.predict(X_test)\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    accuracy,precision,recall,f1 = evaluate_classfier_quality(predicted,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AzJ5JNXghhe6",
    "outputId": "20cf0a1e-42e9-4f77-f654-585ba47d83aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training neural network for Cluster id: 1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "num_of_classes 47\n",
      "Train on 10761 samples, validate on 4612 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 3.1495 - acc: 0.1893 - val_loss: 2.8451 - val_acc: 0.2214\n",
      "Epoch 2/10\n",
      " - 3s - loss: 2.8961 - acc: 0.2120 - val_loss: 2.8366 - val_acc: 0.2214\n",
      "Epoch 3/10\n",
      " - 3s - loss: 2.8801 - acc: 0.2142 - val_loss: 2.8329 - val_acc: 0.2214\n",
      "Epoch 4/10\n",
      " - 3s - loss: 2.8751 - acc: 0.2146 - val_loss: 2.8306 - val_acc: 0.2214\n",
      "Epoch 5/10\n",
      " - 3s - loss: 2.8663 - acc: 0.2143 - val_loss: 2.8446 - val_acc: 0.2214\n",
      "Epoch 6/10\n",
      " - 3s - loss: 2.8661 - acc: 0.2143 - val_loss: 2.8230 - val_acc: 0.2214\n",
      "Epoch 7/10\n",
      " - 3s - loss: 2.8567 - acc: 0.2145 - val_loss: 2.8341 - val_acc: 0.2214\n",
      "Epoch 8/10\n",
      " - 3s - loss: 2.8543 - acc: 0.2145 - val_loss: 2.8172 - val_acc: 0.2214\n",
      "Epoch 9/10\n",
      " - 3s - loss: 2.8516 - acc: 0.2145 - val_loss: 2.8180 - val_acc: 0.2214\n",
      "Epoch 10/10\n",
      " - 3s - loss: 2.8461 - acc: 0.2145 - val_loss: 2.8142 - val_acc: 0.2214\n",
      "Accuracy: 0.2214\n",
      "Precision: 0.0490\n",
      "Recall: 0.2214\n",
      "F1: 0.0803\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 6\n",
      "num_of_classes 43\n",
      "Train on 3607 samples, validate on 1546 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 3.3127 - acc: 0.2118 - val_loss: 2.5093 - val_acc: 0.2555\n",
      "Epoch 2/10\n",
      " - 1s - loss: 2.5369 - acc: 0.2318 - val_loss: 2.4714 - val_acc: 0.2555\n",
      "Epoch 3/10\n",
      " - 1s - loss: 2.4947 - acc: 0.2448 - val_loss: 2.4647 - val_acc: 0.2180\n",
      "Epoch 4/10\n",
      " - 1s - loss: 2.4695 - acc: 0.2503 - val_loss: 2.4610 - val_acc: 0.2180\n",
      "Epoch 5/10\n",
      " - 1s - loss: 2.4661 - acc: 0.2467 - val_loss: 2.4608 - val_acc: 0.2180\n",
      "Epoch 6/10\n",
      " - 1s - loss: 2.4646 - acc: 0.2348 - val_loss: 2.4602 - val_acc: 0.2555\n",
      "Epoch 7/10\n",
      " - 1s - loss: 2.4588 - acc: 0.2418 - val_loss: 2.4447 - val_acc: 0.2555\n",
      "Epoch 8/10\n",
      " - 1s - loss: 2.4507 - acc: 0.2404 - val_loss: 2.4480 - val_acc: 0.2180\n",
      "Epoch 9/10\n",
      " - 1s - loss: 2.4389 - acc: 0.2451 - val_loss: 2.4397 - val_acc: 0.2581\n",
      "Epoch 10/10\n",
      " - 1s - loss: 2.4394 - acc: 0.2487 - val_loss: 2.4414 - val_acc: 0.2555\n",
      "Accuracy: 0.2555\n",
      "Precision: 0.0656\n",
      "Recall: 0.2555\n",
      "F1: 0.1044\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 4\n",
      "num_of_classes 23\n",
      "Train on 1779 samples, validate on 763 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 2.7803 - acc: 0.6582 - val_loss: 1.4171 - val_acc: 0.7287\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3724 - acc: 0.7032 - val_loss: 1.0906 - val_acc: 0.7287\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2437 - acc: 0.7038 - val_loss: 1.1016 - val_acc: 0.7287\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2260 - acc: 0.7038 - val_loss: 1.0856 - val_acc: 0.7287\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2193 - acc: 0.7038 - val_loss: 1.0640 - val_acc: 0.7287\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.2188 - acc: 0.7038 - val_loss: 1.0892 - val_acc: 0.7287\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.2166 - acc: 0.7038 - val_loss: 1.0600 - val_acc: 0.7287\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.2154 - acc: 0.7038 - val_loss: 1.0634 - val_acc: 0.7287\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.2023 - acc: 0.7038 - val_loss: 1.0570 - val_acc: 0.7287\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.2022 - acc: 0.7038 - val_loss: 1.0592 - val_acc: 0.7287\n",
      "Accuracy: 0.7287\n",
      "Precision: 0.5310\n",
      "Recall: 0.7287\n",
      "F1: 0.6143\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 0\n",
      "num_of_classes 53\n",
      "Train on 7590 samples, validate on 3253 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 2.3405 - acc: 0.6014 - val_loss: 1.8481 - val_acc: 0.5967\n",
      "Epoch 2/10\n",
      " - 2s - loss: 1.8247 - acc: 0.6112 - val_loss: 1.8407 - val_acc: 0.5967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 1.8172 - acc: 0.6112 - val_loss: 1.8366 - val_acc: 0.5967\n",
      "Epoch 4/10\n",
      " - 2s - loss: 1.8066 - acc: 0.6112 - val_loss: 1.8341 - val_acc: 0.5967\n",
      "Epoch 5/10\n",
      " - 2s - loss: 1.7928 - acc: 0.6112 - val_loss: 1.8264 - val_acc: 0.5967\n",
      "Epoch 6/10\n",
      " - 2s - loss: 1.7900 - acc: 0.6112 - val_loss: 1.8268 - val_acc: 0.5967\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.7870 - acc: 0.6112 - val_loss: 1.8203 - val_acc: 0.5967\n",
      "Epoch 8/10\n",
      " - 2s - loss: 1.7802 - acc: 0.6112 - val_loss: 1.8190 - val_acc: 0.5967\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.7792 - acc: 0.6112 - val_loss: 1.8185 - val_acc: 0.5967\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.7786 - acc: 0.6112 - val_loss: 1.8197 - val_acc: 0.5967\n",
      "Accuracy: 0.5967\n",
      "Precision: 0.3560\n",
      "Recall: 0.5967\n",
      "F1: 0.4460\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 3\n",
      "num_of_classes 10\n",
      "Train on 855 samples, validate on 367 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 2.7876 - acc: 0.8269 - val_loss: 1.9475 - val_acc: 0.9564\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.7259 - acc: 0.9743 - val_loss: 0.6497 - val_acc: 0.9564\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.4063 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.4147 - acc: 0.9743 - val_loss: 0.7027 - val_acc: 0.9564\n",
      "Accuracy: 0.9564\n",
      "Precision: 0.9147\n",
      "Recall: 0.9564\n",
      "F1: 0.9351\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 7\n",
      "num_of_classes 9\n",
      "Train on 329 samples, validate on 141 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 2.8597 - acc: 0.5653 - val_loss: 2.7284 - val_acc: 0.9362\n",
      "Epoch 2/10\n",
      " - 0s - loss: 2.5903 - acc: 0.9240 - val_loss: 2.0800 - val_acc: 0.9362\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.6607 - acc: 0.9240 - val_loss: 0.5614 - val_acc: 0.9362\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6096 - acc: 0.9240 - val_loss: 0.6136 - val_acc: 0.9362\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.7415 - acc: 0.9240 - val_loss: 0.7323 - val_acc: 0.9362\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.8025 - acc: 0.9240 - val_loss: 0.5978 - val_acc: 0.9362\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.5665 - acc: 0.9240 - val_loss: 0.4445 - val_acc: 0.9362\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.4210 - acc: 0.9240 - val_loss: 0.4301 - val_acc: 0.9362\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.4468 - acc: 0.9240 - val_loss: 0.4908 - val_acc: 0.9362\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.4838 - acc: 0.9240 - val_loss: 0.4414 - val_acc: 0.9362\n",
      "Accuracy: 0.9362\n",
      "Precision: 0.8764\n",
      "Recall: 0.9362\n",
      "F1: 0.9053\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 2\n",
      "num_of_classes 3\n",
      "Train on 596 samples, validate on 256 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.6752 - acc: 0.8188 - val_loss: 1.2608 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6255 - acc: 0.9983 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0280 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9983 - val_loss: 5.9605e-07 - val_acc: 1.0000\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1: 1.0000\n",
      "\n",
      "\n",
      "Training neural network for Cluster id: 5\n",
      "num_of_classes 7\n",
      "Train on 366 samples, validate on 158 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 2.5989 - acc: 0.6284 - val_loss: 2.4747 - val_acc: 0.9557\n",
      "Epoch 2/10\n",
      " - 0s - loss: 2.2767 - acc: 0.9672 - val_loss: 1.7764 - val_acc: 0.9557\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2255 - acc: 0.9672 - val_loss: 0.3412 - val_acc: 0.9557\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2845 - acc: 0.9672 - val_loss: 0.5096 - val_acc: 0.9557\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.4360 - acc: 0.9672 - val_loss: 0.7124 - val_acc: 0.9557\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.5204 - acc: 0.9672 - val_loss: 0.7141 - val_acc: 0.9557\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.5285 - acc: 0.9672 - val_loss: 0.7141 - val_acc: 0.9557\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.5254 - acc: 0.9672 - val_loss: 0.7141 - val_acc: 0.9557\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.5213 - acc: 0.9672 - val_loss: 0.7141 - val_acc: 0.9557\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.5285 - acc: 0.9672 - val_loss: 0.7141 - val_acc: 0.9557\n",
      "Accuracy: 0.9557\n",
      "Precision: 0.9134\n",
      "Recall: 0.9557\n",
      "F1: 0.9340\n"
     ]
    }
   ],
   "source": [
    "run_deep_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuE3Hz35voOx"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] I. Dabbura, “K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks,” Medium, 03-Sep-2019. [Online]. Available: https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a.\n",
    "\n",
    "[2] kk7nc, “kk7nc/Text_Classification,” GitHub, 05-Dec-2019. [Online]. Available: https://github.com/kk7nc/Text_Classification#autoencoder.\n",
    "\n",
    "[3] F. Chollet, “The Keras Blog,” The Keras Blog ATOM. [Online]. Available: https://blog.keras.io/building-autoencoders-in-keras.html.\n",
    "\n",
    "[4] Liang, Hong et al. “Text feature extraction based on deep learning: a review.” EURASIP journal on wireless communications and networking vol. 2017,1 (2017): 211. doi:10.1186/s13638-017-0993-1\n",
    "\n",
    "[5] Maggipinto, M., Masiero, C., Beghi, A., & Susto, G. A. (2018). A Convolutional Autoencoder Approach for Feature Extraction in Virtual Metrology: Paper ID 259. Procedia Manufacturing, 17, 126-133.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledging Help from TA\n",
    "\n",
    "We would like to thank Mohammad Etemad for his feedback and support in this project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
